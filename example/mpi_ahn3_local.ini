
[INPUT_OUTPUT]
# The input directory type. remote: the input directory is on a remote file system not accessible to the compute nodes and input files are copied to a temporary directory. local: the input directory is on a local file system accessible to the compute nodes and input files are not copied to a temporary directory.
InputDirType = local

# The output directory type. remote: the output directory is on a remote file system not accessible to the compute nodes. local: the output directory is on a local file system accessible to the compute nodes
OutputDirType = local

# The input directory containing the laz files.
InputDir = /storage/nD_PointClouds/data/AHN3/raw/rdam

# The output directory for the converted data. The directory is created if it does not exist.
OutputDir = /storage/nD_PointClouds/data/AHN3/potree_mpi/rdam/out

# The directory that contains the headers for the laz files. This is used to determine the bounding box of the input data. The name of the headers files should be same as the laz files and must have ".json" extension.
LazHeadersDir = /storage/nD_PointClouds/data/AHN3/raw/rdam


[COPIER]
# The type of the copier. cp: uses cp to copy files. Only cp is supported at the moment.
Type = cp


[TMP_STORAGE]
# The maximum temporary space available in bytes. Use python expression format For example 600*(1024)*3
MaxTmpSpaceAvailable = 90000000000 #110*(1024)*3

# The directory to store the temporary files. The directory is created if it does not exist.
TmpDir = /storage/nD_PointClouds/data/AHN3/potree_mpi/rdam/tmp

# The amount copied from the input directory to the temporary directory in bytes for the the counting phase of the converter. Use python expression format. For example 20*(1024)**3
CountingBatchSize = 20*(1024)**3

# The amount copied from the input directory to the temporary directory in bytes for the the distribution phase of the converter. Use python expression format.
DistributionBatchSize = 20*(1024)**3

# The expected compression ratio of the input data. This is used to estimate the required disk space for the temporary and output files.
LazCompressionRatio = 7


[PARTITIONS]
# The file that contains the partition ins CSV format. The file should have the following columns: "bladnr" and "partition_id". The "bladnr" column should contain the name of the laz file or just the bladnr identifier and the "partition_id" column should contain the partition number of the laz file. The partition_id should be a number between 0 and the number of partitions - 1. For un-partitioned data, all the files should have the same partition_id = 0. A example file is provided in the "PotreeConverterPartitioned/partitions/ahn3_partitions_8x8.csv"
CSV = /storage/nD_PointClouds/ndpc_PotreeConverter/examples/ahn3_local_partitions.csv


[PROGRAM]
# The path to the converter program.
Path = /storage/nD_PointClouds/ndpc_PotreeConverter/build/PotreeConverterMPI
Name = PotreeConverterMPI
# The options to pass to the converter program. The following options are supported:
# --encoding: Encoding type "BROTLI", "UNCOMPRESSED"(default)
# -m, --method: Point sampling method "poisson"(default), "poisson_average", "random"
# --attributes: Attributes in the output file. If not specified, all attributes are included in the output file.
# --threads: Number of threads to use
# --bounds: Bounds of the pointcloud to be converted. The format should be: [minx,maxx],[miny,maxy],[minz,maxz]. If not provided, the bounds will be computed from the input files.
# --max-mem: Maximum memory to be used by the program in GB. If not provided, the program will use all the available memory.
Options = --encoding BROTLI --max-mem 90 --bounds [80000.0,90000.0],[431250.0,443750.0],[-50.0,250.0]


[SCHEDULER]
# The scheduler type. slurm: SLURM scheduler. pbs: PBS scheduler. local: program is run locally.
Type = local

# The parameters to pass to the scheduler. See the scheduler documentation for the parameters.
Parameters = --job-name=PotreeConverterMPI_ahn3_rdam_local --output=/storage/nD_PointClouds/data/AHN3/potree_mpi/rdam/sbatchOutput/PotreeConverterMPI_ahn3_rdam_local_%%j.out --error=/storage/nD_PointClouds/data/AHN3/potree_mpi/rdam/sbatchOutput/PotreeConverterMPI_ahn3_rdam_local_%%j.err --partition=compute --account=research-abe-aet --nodes=1 --ntasks-per-node=1 --cpus-per-task=12 --time=100:00:00 --mem=12G